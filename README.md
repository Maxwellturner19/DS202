# DS202
This is my DS202 (Data Science for Social Scientists) LSE module workings

Part 01

The first half of the course focuses on the fundamentals of machine learning algorithms, with an emphasis on supervised learning.

Lecturer:

Photo of Dr Jon Cardoso Silva
Dr. Jon Cardoso-Silva
ğŸ—“ï¸ Week 01
15 Jan 2024 -
19 Jan 2024

ğŸ’» Lab

R/RStudio + tidyverse recap

To fully prepare for this lab, we highly recommend you go through the setup steps outlined in section 1 of the ğŸ“‹ Getting Ready page.

ğŸ§‘â€ğŸ« Lecture

Introduction, Course Logistics & R programming

ğŸ“– Revise

 Click to see if youâ€™re caught up
ğŸ—“ï¸ Week 02
22 Jan 2024 -
26 Jan 2024

ğŸ’» Lab

Practice data manipulation with dplyr and tidyr

ğŸ§‘â€ğŸ« Lecture

Supervised Learning: Introduction to Regression Algorithms

What is Supervised Learning? What is Regression?
Algorithm: Linear Regression (simple and multiple)
ğŸ“£ Assignment Reveal

To help you familiarise yourself with the style of the assignments, we will announce a formative (practice) assignment this week.

This assignment will be about dplyr and tidyr. The precise requirements will be announced in the lecture.
You will submit via GitHub Classroom
ğŸ—“ï¸ Week 03
29 Jan 2024 -
02 Feb 2024

ğŸ’» Lab

Linear regression, a tidymodels tutorial

ğŸ§‘â€ğŸ« Lecture

Supervised Learning: Fundamentals of Classification

What is classification?
Algorithm: Logistic Regression
From binary to multi-class classification
âŒ› Deadline

Your first formative will be due a day before the lecture!

ğŸ“£ Assignment Reveal

Your Summative 01, worth 10% of your final grade will be announced in the lecture of this week.

ğŸ—“ï¸ Week 04
05 Feb 2024 -
09 Feb 2024

ğŸ’» Lab

Tidymodel recipes and workflows - a tutorial

ğŸ§‘â€ğŸ« Lecture

Supervised Learning: Resampling methods

How to evaluate a model?
What is overfitting?
What is resampling?
Method: Train-Test Split
Method: Cross-Validation
Method: The Bootstrap
Method: Hyperparameter Tuning
ğŸ†˜ Drop-in sessions

We will host drop-in sessions on Week 04 to help support you with your Summative 01.

âŒ› Deadline

Your Summative 01 will be due sometime this week. The exact deadline will be announced in the lecture of Week 03.

ğŸ—“ï¸ Week 05
12 Feb 2024 -
16 Feb 2024

ğŸ’» Lab

Parameter tuning with tidymodels

ğŸ§‘â€ğŸ« Lecture

Supervised Learning: Non-linear algorithms and ensemble methods

What is non-linearity?
Why canâ€™t linear models capture non-linearity?
Algorithm: Decision Trees
Algorithm: Random Forests
Algorithm: Support Vector Machines
Overview of other algorithms: k-Nearest Neighbours, Neural Networks, etc.
ğŸ“£ Assignment Reveal

Your Summative 02, worth 20% of your final grade will be announced in the lecture of this week.

ğŸ—“ï¸ Week 06
19 Feb 2024 -
23 Feb 2024

ğŸ†˜ Drop-in sessions

There is no lecture or lab this week. Instead, we will hold drop-in sessions to help you with your Summative 02. The exact times and dates will be announced in the lecture of Week 05.

Part 02

In the second half of the course, the focus shifts to unsupervised learning.

Lecturer:

Photo of Dr Ghita Berrada
Dr. Ghita Berrada
ğŸ—“ï¸ Week 07
26 Feb 2024 -
01 Mar 2024

ğŸ’» Lab

Decision trees and further parameter tuning

ğŸ§‘â€ğŸ« Lecture

Unsupervised Learning: Introduction and clustering part 1

What is unsupervised learning? How does it differ from supervised learning?
What is clustering?
Algorithm: k-means
Variants of k-means
âŒ› Deadline

Your Summative 02 will be due sometime this week. The exact deadline will be announced in the lecture of Week 05.

ğŸ—“ï¸ Week 08
04 Mar 2024 -
08 Mar 2024

ğŸ’» Lab

Comparing models and model evaluation

ğŸ§‘â€ğŸ« Lecture

Unsupervised Learning: Clustering part 2 and Anomaly detection

Another Algorithm for clustering: DBSCAN
Unsupervised learning goes beyond clustering: Outliers/anomalies can be important too! Examples of anomaly detection use cases
Algorithm: Anomaly detection through clustering (e.g., k-means)
Algorithm: Anomaly detection through density estimation (Local outlier factor (LOF))
ğŸ—“ï¸ Week 09
11 Mar 2024 -
15 Mar 2024

ğŸ’» Lab

Unsupervised Learning: Obtaining Insights via Clustering

ğŸ§‘â€ğŸ« Lecture

Unsupervised Learning: Dimensionality reduction

What is dimensionality reduction, and why is it useful?
Algorithm: PCA
Algorithm: UMAP
What is the distinction between clustering (e.g k-means) and dimensionality reduction (e.g PCA)?
ğŸ“£ Assignment Reveal

In this weekâ€™s lecture, we will announce your Summative 03, worth 30% of your final grade.

Part 03

Finally, you will be introduced to the basics of text mining, and then we will look at some applications of the algorithms weâ€™ve learned so far.

ğŸ—“ï¸ Week 10
18 Mar 2024 -
22 Mar 2024

ğŸ’» Lab

Tutorial of dimensionality reduction and anomaly detection

ğŸ§‘â€ğŸ« Lecture

Applications: Text as Data & Topic Modelling

ğŸ—“ï¸ Week 11
25 Mar 2024 -
29 Mar 2024

ğŸ’» Lab

A tutorial of quanteda

ğŸ§‘â€ğŸ« Lecture

Applications: Predictive Modelling on Tabular Data, a walkthrough

âŒ› Deadline

Your Summative 03 will be due sometime this week or the week following the end of the term. The exact deadline will be announced in the lecture of Week 09.

ğŸ†˜ Drop-in sessions

We will host drop-in sessions on Week 11 to help support you with your Summative 03.
